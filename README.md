# DS5110 - Final Project
End-to-end data science project: Wildfire analysis and prediction using an SQL database and exploratory data analysis.

By: Julian Baker & Konstantinos Theodoropoulos

Project Overview

This project explores how large-scale wildfire data from multiple countries can be merged, cleaned, and analyzed within a unified SQL database.
Our goals were to:
- Combine U.S. and Canadian wildfire datasets into a single, structured database
- Perform exploratory data analysis and advanced SQL reporting
- Evaluate how a unified international database can support wildfire prediction and environmental insights
- Build a reproducible analysis pipeline (SQL + Python + EDA + ML)
While our early prediction models did not produce high accuracy, the project demonstrates the importance of standardized wildfire data and provides a strong foundation for future modeling.

Instructions

1. Download the datasets from the following links:
- U.S. Wildfire Dataset: https://www.fs.usda.gov/rds/archive/catalog/RDS-2013-0009.6 
- Canadian National Fire Database (NFDB) dataset: https://cwfis.cfs.nrcan.gc.ca/datamart/download/nfdbpnt 

2. Data Merging & Preprocessing
- Download the IPYNB file into Google Drive and place it alongside the datasets. 
- Change the paths in the code to match where the files are located. For example, in the data merging file, change this line:
df_canada = pd.read_csv('/content/gdrive/MyDrive/DS 5110 Final Project/Data to use/NFDB_point_20240613.csv')
to the path of where the data is located in your drive. The results should appear when you first load in the IPYNB files.

3. Recreating the SQL Database
- Load the SQL schema (schema.sql) into SQLite
- Import the merged dataset generated by the notebook to SQLite
- Any of the 8 SQL queries can be executed by running its respective file

Note: The datasets and the full SQLite database file are not included in this repository because they exceed GitHubâ€™s 25MB upload limit.
